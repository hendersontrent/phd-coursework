---
title: OLET5610 Report
authors:
  - name: Trent Henderson
    email: then6675@uni.sydney.edu.au
bibliography: references.bib
biblio-style: unsrt
output: 
  rticles::arxiv_article:
    keep_tex: true
---

# Method

Time-series classification algorithms are typically benchmarked using the Time Series Classification Repository, which contains 128 univariate (i.e., values sampled uniformly in time) time-series classification datasets/problems and 30 multivariate (i.e., for any time $t$, $\mathbf{y}_{t} = (y_{1t},\ldots, y_{nt})$ describes $n$ realisations at time $t$) datasets [@UEA_UCR_Repo]. Here, we focus on the univariate setting. Many algorithms developed across the sciences have been evaluated across the standardised univariate datasets provided in the repository, with black-box algorithms typically outperforming competitors [@bagnallGreatTimeSeries2017]. Given the growing need for algorithmic transparency in the sciences and industry, the present research aims to explore the feasibility of a new approach using one of the 128 datasets as a test case. Specifically, this work aims to determine if using informative summary statistics (known as "features"), can be used to construct high performing classification procedures that rival or outperform existing approaches [@fulcherHighlyComparativeTimeseries2013, @fulcherFeaturebasedTimeseriesAnalysis2017, fulcherFeatureBasedTimeSeriesAnalysis2018]. Examples of such features include properties of the time-series distribution of values, autocorrelation structure, entropy, model-fit statistics, nonlinear time-series analysis, stationarity, and many others [@fulcherHighlyComparativeFeaturebased2014]. At its core, feature-based time-series analysis reduces an time series $\times$ time matrix to a time series $\times$ feature matrix, which can then be used for statistical learning (such as classification), using the interpretative values of time-series features as inputs.

## Dataset

The "Chinatown" dataset in the Time Series Classification Repository contains data collected by The City of Melbourne from its automated pedestrian counting system [@CityMelb]. The goal of this system is to better understand pedestrian activity within the city, an enable city planners and decision makers to understand various important questions, such as how people use different city locations at different time of the day. The reduced version of this dataset contained on the Time Series Classification Repository contains an extraction of 10 locations for the whole year of 2017, such that the data is a collection of pedestrian counts across 2017. Labels for two classes are provided in the data: **Weekend** and **Weekday**^[https://www.timeseriesclassification.com/description.php?Dataset=Chinatown]. The goal of this problem is to predict class membership from the time-series values. The dataset contains a pre-designated train-test split, with 20 samples in the train set (each of length $T = 24$), and 345 samples in the test set (each of length $T = 24$).

## Algorithmic Approach

The approach in this work contains three stages: (i) extraction of time-series features for each unique time series; (ii) dimensionality reduction to obtain a reduced set of informative vectors; (iii) classification using the dimensionality reduction results as inputs into a random forest classifier. Features from four open-source feature sets (**catch22** [@lubbaCatch22CAnonicalTimeseries2019, @Rcatch22], **tsfresh** [@christTimeSeriesFeatuRe2018], **TSFEL** [@barandasTSFELTimeSeries2020], and **Kats** [@Kats]) will be extracted using the R package "theft" [@theft]. Given the $T = 24$ length of each time series, **catch22** will extract 24 features (as mean and standard deviation are added to the standard 22 features), **tsfresh** will extract 779 features, **TSFEL** will extract 147 features, and **Kats** will extract 40 features. This will produce a resulting time series $\times$ feature matrix of size 365 $\times$ 990. Given the high within-set redundancy (i.e., high absolute correlations between features in a set) observed for particularly the **tsfresh** and **TSFEL** feature sets in previous work, dimensionality reduction through principal components analysis (PCA) will then be applied to substantially reduce the input matrix size for the classification algorithm into a time series $\times$ principal component matrix [@hendersonEmpiricalEvaluationTimeSeries2021]. A threshold of 80$\%$ cumulative variance explained will be used to determine the principal components to retain. Following the procedure of previous work [@ruizGreatMultivariateTime2021], the methodology will then train and evaluate the accuracy of the classifier over 30 resamples of train-test splits, where each is seeded for reproducibility and the first is always the pre-designated train-test split in the data as it comes from the Time Series Classification Repository. This will enable scientific inference of algorithmic performance with uncertainty and facilitate a direct comparison with the performance of existing methods.

## Hypotheses

Given the existence of the Time Series Classification Repository [@UEA_UCR_Repo] which holds the Chinatown dataset (whose sole purpose is to facilitate benchmarking of time-series classification algorithms), it is expected that meaningful class separation will be possible. The present research takes a novel approach of chaining time-series feature extraction into dimensionality reduction techniques into a classification algorithm --- an approach that has seen almost no research attention to-date. As such, a primary goal of this work is to understand the performance of this approach relative to current benchmarks. 

Given the high within-set redundancy (i.e., high absolute correlations between features in a set) observed for particularly the **tsfresh** and **TSFEL** feature sets in previous work, it is hypothesised that dimensionality reduction techniques will substantially reduce the input matrix size for the classification algorithm from the original time series $\times$ feature matrix to a time series $\times$ principal component matrix [@hendersonEmpiricalEvaluationTimeSeries2021]. Further, it is also hypothesised that using the time series $\times$ principal component matrix as input to a classification algorithm (i.e., random forest classifier) will not result in a substantial reduction in classification performance compared to using the time series $\times$ feature matrix.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Load packages

library(dplyr)
library(tidyr)
library(ggplot2)
library(foreign)
library(factoextra)
library(ggpubr)
library(broom)
library(janitor)
library(caret)

# Load the dataset (pre-run in another script to avoid Python/Rmarkdown issues)

load("tmp.Rda")
load("outs.Rda")

# Get feature matrix size to highlight dimensionality

#print(paste0(length(unique(outs$id)), " x ", length(unique(outs$names))))
```

# Results

XX

See Figure \@ref(fig:tsplots).

```{r tsplots, fig.cap = "Raw time-series plots of two randomly selected time series from each class ("Weekend" and "Weekday"). Small differences in temporal dynamics are visible.", echo = FALSE, warning = FALSE, message = FALSE}
# Plot some raw time series examples (two from each class)

set.seed(123)

weekend_ids <- tmp %>%
  filter(target == "1") %>%
  dplyr::select(id) %>%
  distinct() %>%
  pull(id) %>%
  sample(size = 2)

weekday_ids <- tmp %>%
  filter(target == "2") %>%
  dplyr::select(id) %>%
  distinct() %>%
  pull(id) %>%
  sample(size = 2)

tsplot <- tmp %>%
  filter(id %in% append(weekend_ids, weekday_ids)) %>%
  dplyr::mutate(target = ifelse(.data$target == "1", "Weekend", "Weekday")) %>%
  dplyr::mutate(target = factor(.data$target, levels = c("Weekend", "Weekday")),
                id = paste0("ID: ", id)) %>%
  ggplot(aes(x = timepoint, y = values, colour = target)) +
  geom_line(size = 1.15) +
  geom_point(size = 2) +
  labs(title = "Raw time series samples from both classes",
       x = "Time",
       y = "Value",
       colour = NULL) +
  scale_colour_brewer(palette = "Dark2") +
  theme_bw() +
  theme(legend.position = "bottom",
        panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"),
        strip.background = element_blank(),
        strip.text = element_text(face = "bold")) +
  facet_wrap(~id)

print(tsplot)
```

## Dimensionality reduction

XX

See Figure \@ref(fig:eigenplots).

```{r eigenplots, fig.cap = "Summary of nineteen retained principal components. (A) Percentage of variance explained is plotted in descending order for each of the retained principal components. (B) Cumulative variance explained is plotted for each of the retained principal components. An 80% cumulative variance threshold was selected to determine the principal components to retain which returned the nineteen plotted here (from the original 313). (C) Eigenvalues of the nineteen retained principal components are plotted in descending order. All retained components exceed the $\\lambda = 1$ cutoff for the Kaiser criterion.", echo = FALSE, warning = FALSE, message = FALSE, fig.width = 7, fig.height = 8}
#-------
# Do PCA
#-------

# Widen model matrix and fit PCA

mywhere <- function(fn) {
  predicate <- rlang::as_function(fn)
  
  function(x, ...) {
    out <- predicate(x, ...)
    if (!rlang::is_bool(out)) {
      rlang::abort("`where()` must be used with functions that return `TRUE` or `FALSE`.")
    }
    out
  }
}

fit_mat <- outs %>%
  dplyr::select(id, names, values) %>%
  tidyr::pivot_wider(id_cols = "id", names_from = "names", values_from = "values") %>%
  tibble::column_to_rownames(var = "id") %>%
  dplyr::select_if(~sum(!is.na(.)) > 0) %>% # Remove features that are just NA before dropping rows
  tidyr::drop_na() %>%
  dplyr::select(mywhere(~dplyr::n_distinct(.) > 1)) # Keep columns that aren't a constant

# Get feature matrix size to highlight dimensionality

#print(paste0(nrow(fit_mat), " x ", ncol(fit_mat)))

# Fit PCA

fits <- fit_mat %>%
  stats::prcomp(scale = TRUE)

#------------------------
# Assess # of PCs to keep
#------------------------

# Filter to just PCs that contribute just over 80% of the variance

eigs <- as.data.frame(get_eig(fits)) %>%
  tibble::rownames_to_column(var = "dimension") %>%
  mutate(rank = dense_rank(cumulative.variance.percent),
         flag = ifelse(cumulative.variance.percent > 80, TRUE, FALSE)) %>%
  group_by(flag) %>%
  mutate(ranker = dense_rank(cumulative.variance.percent)) %>%
  ungroup() %>%
  mutate(ranker = ifelse(flag == FALSE, 1, ranker)) %>%
  filter(ranker == 1) %>%
  dplyr::select(-c(rank, flag, ranker)) %>%
  mutate(dimension = gsub("Dim.", "\\1", dimension),
         dimension = paste0("PC ", dimension))

# Draw % of variance explained plot

p <- eigs %>%
  ggplot(aes(x = reorder(dimension, -variance.percent), y = variance.percent)) +
  geom_bar(stat = "identity", fill = "#1B9E77", alpha = 0.9) +
  geom_line(group = 1, size = 1) +
  geom_point(size = 2) +
  labs(title = "(A) Percentage of variance explained by principal component",
       x = "Principal component",
       y = "Variance explained (%)") +
  geom_line() +
  geom_point() +
  scale_y_continuous(limits = c(0, 35),
                     breaks = seq(from = 0, to = 40, by = 5),
                     labels = function(x)paste(x, "%")) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"),
        axis.text = element_text(size = 8))

# Draw cumulative variance plot

p1 <- eigs %>%
  ggplot(aes(x = reorder(dimension, cumulative.variance.percent), y = cumulative.variance.percent)) +
  geom_bar(stat = "identity", fill = "#D95F02", alpha = 0.9) +
  geom_line(group = 1, size = 1) +
  geom_point(size = 2) +
  geom_hline(yintercept = 80, lty = "dashed", colour = "black", size = 1) +
  labs(title = "(B) Cumulative variance explained by principal components",
       x = "Principal component",
       y = "Cumulative variance explained (%)") +
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(from = 0, to = 100, by = 20),
                     labels = function(x)paste(x, "%")) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"),
        axis.text = element_text(size = 8))

# Draw eigenvalue plot

p2 <- eigs %>%
  ggplot(aes(x = reorder(dimension, -eigenvalue), y = eigenvalue)) +
  geom_bar(stat = "identity", fill = "#7570B3", alpha = 0.9) +
  geom_hline(yintercept = 1, lty = "dashed", colour = "black", size = 1) +
  labs(title = "(C) Eigenvalues of retained principal components",
       x = "Principal component",
       y = "Eigenvalue") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"),
        axis.text = element_text(size = 8))

# Combine plots

eigenplots <- ggpubr::ggarrange(p, p1, p2, nrow = 3, ncol = 1, common.legend = TRUE, legend = "bottom")
print(eigenplots)
```

XX

See Figure \@ref(fig:biplot).

```{r biplot, fig.cap = "Principal components analysis biplot. The first principal component (positioned along the $x$-axis) explains 30.5% of the variance in the Chinatown dataset. The second principal component (positioned along the $y$-axis) explains 17.1% of the variance in the Chinatown dataset. Despite some overlap, meaningful class separation is visible between the $Weekend$ and $Weekday$ classes.", echo = FALSE, warning = FALSE, message = FALSE}
# Get eigenvalues and summary stats for plot

eigen_summary <- fits %>%
  broom::tidy(matrix = "eigenvalues") %>%
  dplyr::filter(.data$PC %in% c(1, 2)) %>% # Filter to just the 2 going in the plot
  dplyr::select(c(.data$PC, .data$percent)) %>%
  dplyr::mutate(percent = round(.data$percent * 100), digits = 1)

eigen_pc1 <- eigen_summary %>%
  dplyr::filter(.data$PC == 1)

eigen_pc2 <- eigen_summary %>%
  dplyr::filter(.data$PC == 2)

eigen_pc1 <- paste0(eigen_pc1$percent,"%")
eigen_pc2 <- paste0(eigen_pc2$percent,"%")

# Get group data

groups <- outs %>%
  dplyr::rename(group_id = group) %>%
  dplyr::group_by(.data$id, .data$group_id) %>%
  dplyr::summarise(counter = dplyr::n()) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c(.data$counter)) %>%
  dplyr::mutate(id = as.factor(.data$id)) %>%
  dplyr::mutate(group_id = ifelse(.data$group_id == "1", "Weekend", "Weekday")) %>%
  dplyr::mutate(group_id = factor(.data$group_id, levels = c("Weekend", "Weekday")))

# Draw plot

pca_plot <- fits %>%
  broom::augment(fit_mat) %>%
  dplyr::rename(id = 1) %>%
  dplyr::mutate(id = as.factor(.data$id)) %>%
  dplyr::rename(.fitted1 = .data$.fittedPC1,
                .fitted2 = .data$.fittedPC2) %>%
  dplyr::inner_join(groups, by = c("id" = "id")) %>%
  dplyr::mutate(group_id = as.factor(.data$group_id)) %>%
  ggplot2::ggplot(ggplot2::aes(x = .data$.fitted1, y = .data$.fitted2)) +
  ggplot2::stat_ellipse(ggplot2::aes(x = .data$.fitted1, y = .data$.fitted2, fill = .data$group_id), geom = "polygon", alpha = 0.2) +
  ggplot2::guides(fill = "none") +
  ggplot2::scale_fill_brewer(palette = "Dark2") +
  ggplot2::geom_point(size = 2, ggplot2::aes(colour = .data$group_id)) + 
  ggplot2::labs(title = "Principal components analysis biplot",
                x = paste0("PC 1"," (", eigen_pc1, ")"),
                y = paste0("PC 2"," (", eigen_pc2, ")"),
                colour = NULL) +
  ggplot2::scale_colour_brewer(palette = "Dark2") +
  ggplot2::theme_bw() +
  ggplot2::theme(panel.grid.minor = ggplot2::element_blank(),
                 legend.position = "bottom",
                 plot.title = element_text(face = "bold"),
                 axis.text = element_text(size = 8),
                 axis.title = element_text(size = 9))

print(pca_plot)
```

XX

## Time-series classification

XX

```{r, echo = FALSE, warning = FALSE, message = FALSE}
#--------------- Helper functions ----------------

#-----------------------
# Classification metrics
#-----------------------

# Recall (for use in computing balanced classification accuracy)

calculate_recall <- function(matrix, x){
  tp <- as.numeric(matrix[x, x])
  fn <- sum(matrix[x, -x])
  
  # Add a catch for when 0s occupy the entire row in the confusion matrix
  # NOTE: Is this the correct way to handle? Seems consistent with {caret}'s default matrix
  
  if(tp + fn == 0){
    recall <- 0
  } else{
    recall <- tp / (tp + fn)
  }
  return(recall)
}

# Four MECE parts of the confusion matrix (TP, FP, TN, FN)

calculate_cm_stats <- function(matrix, x){
  tp <- as.numeric(matrix[x, x])
  fp <- sum(matrix[-x, x])
  tn <- sum(matrix[-x, -x])
  fn <- sum(matrix[x, -x])
  mymat <- matrix(c(tp, fp, tn, fn), nrow = 1, ncol = 4)
  return(mymat)
}

#-------------------------------------
# Calculate balanced accuracy in caret
#-------------------------------------

calculate_balanced_accuracy <- function(data, lev = NULL, model = NULL) {
  
  # Calculate balanced accuracy from confusion matrix as the average of class recalls as per https://arxiv.org/pdf/2008.05756.pdf
  
  cm <- t(as.matrix(caret::confusionMatrix(data$pred, data$obs)$table))
  
  recall <- 1:nrow(cm) %>%
    purrr::map(~ calculate_recall(cm, x = .x)) %>%
    unlist()
  
  balanced_accuracy <- sum(recall) / length(recall)
  
  # Calculate accuracy
  
  accuracy <- sum(diag(cm)) / sum(cm)
  
  # Return results
  
  out <- c(accuracy, balanced_accuracy)
  names(out) <- c("Accuracy", "Balanced_Accuracy")
  return(out)
}

#-----------------------
# Resample model fitting
#-----------------------

#' Function to fit models over 30 resamples as per Bagnall et al. and save results where first sample is always original train-test split
#' @param data the dataframe of feature results to operate on
#' @param train_rows number of cases in the train set
#' @param test_rows number of cases in the test set
#' @param train_groups dataframe containing proportions of each class in original train split
#' @param test_groups dataframe containing proportions of each class in original test split
#' @param x the resample number to control seed for reproducibility
#' @param test_method the algorithm to use for quantifying class separation. Defaults to \code{"gaussprRadial"}
#' @param use_balanced_accuracy a Boolean specifying whether to use balanced accuracy as the summary metric for caret model training. Defaults to \code{FALSE}
#' @param use_k_fold a Boolean specifying whether to use k-fold procedures for generating a distribution of classification accuracy estimates. Defaults to \code{TRUE}
#' @param num_folds an integer specifying the number of folds (train-test splits) to perform if \code{use_k_fold} is set to \code{TRUE}. Defaults to \code{10}
#' @returns an object of class dataframe
#' @author Trent Henderson
#' 

fit_resamples <- function(data, train_rows, test_rows, train_groups, test_groups, x, 
                          test_method, use_balanced_accuracy, use_k_fold, num_folds){
  
  message(paste0("Fitting model ", x))
  set.seed(x)
  
  #----------------- Data preparation and resampling ------------------
  
  if(x == 1){
    
    # Set up train and test data
    
    tmp_train <- data %>%
      dplyr::filter(.data$set_split == "Train") %>%
      dplyr::select(-c(.data$set_split)) %>%
      dplyr::select(-c(.data$method)) %>%
      tidyr::pivot_wider(id_cols = c("id", "group"), names_from = "names", values_from = "values") %>%
      dplyr::select(-c(.data$id))
    
    tmp_test <- data %>%
      dplyr::filter(.data$set_split == "Test") %>%
      dplyr::select(-c(.data$set_split)) %>%
      dplyr::select(-c(.data$method)) %>%
      tidyr::pivot_wider(id_cols = c("id", "group"), names_from = "names", values_from = "values") %>%
      dplyr::select(-c(.data$id))
    
  } else{
    
    # Randomly allocate the correct number of each class to train and test as manual stratified sample
    
    resampled <- list()
    
    for(i in unique(data$group)){
      feasible_ids <- data %>%
        dplyr::filter(group == i) %>%
        dplyr::select(c(id)) %>%
        dplyr::distinct() %>%
        dplyr::pull(id)
      
      n <- train_groups %>%
        dplyr::filter(group == i) %>%
        pull(counter)
      
      traindata <- data.frame(id = sample(feasible_ids, size = n)) %>%
        dplyr::mutate(set_split_new = "Train")
      
      testdata <- data.frame(id = feasible_ids[!feasible_ids %in% traindata$id]) %>%
        dplyr::mutate(set_split_new = "Test")
      
      stopifnot((nrow(traindata) + nrow(testdata)) == length(feasible_ids))
      
      joined <- dplyr::bind_rows(traindata, testdata)
      resampled[[i]] <- joined
    }
    
    resampled <- do.call(rbind, resampled)
    rownames(resampled) <- c()
    
    # Properly set up train and test data
    
    newdata <- data %>%
      dplyr::left_join(resampled, by = c("id" = "id"))
    
    tmp_train <- newdata %>%
      dplyr::filter(.data$set_split_new == "Train") %>%
      dplyr::select(c(.data$id, .data$group, .data$names, .data$values)) %>%
      tidyr::pivot_wider(id_cols = c("id", "group"), names_from = "names", values_from = "values") %>%
      dplyr::select(-c(.data$id))
    
    tmp_test <- newdata %>%
      dplyr::filter(.data$set_split_new == "Test") %>%
      dplyr::select(c(.data$id, .data$group, .data$names, .data$values)) %>%
      tidyr::pivot_wider(id_cols = c("id", "group"), names_from = "names", values_from = "values") %>%
      dplyr::select(-c(.data$id))
  }
  
  #----------------- Model fitting and returns ------------------
  
  # Fit models
  
  if(use_k_fold){
    
    # Train model
    
    if (use_balanced_accuracy) {
      fitControl <- caret::trainControl(method = "cv",
                                        number = num_folds,
                                        summaryFunction = calculate_balanced_accuracy,
                                        classProbs = TRUE)
    } else {
      fitControl <- caret::trainControl(method = "cv",
                                        number = num_folds,
                                        classProbs = TRUE)
    }
    
    mod <- caret::train(group ~ .,
                        data = tmp_train,
                        method = test_method,
                        trControl = fitControl,
                        preProcess = c("center", "scale", "nzv"))
    
    # Print variable importance summary
    
    importance <<- c(importance, caret::varImp(mod, scale = FALSE))
    
    # Get main predictions
    
    u <- dplyr::union(predict(mod, newdata = tmp_test), tmp_test$group)
    mytable <- table(factor(stats::predict(mod, newdata = tmp_test), u), factor(tmp_test$group, u))
    cm <- t(as.matrix(caret::confusionMatrix(mytable)$table))
    
    if(use_balanced_accuracy){
      
      recall <- 1:nrow(cm) %>%
        purrr::map(~ calculate_recall(cm, x = .x)) %>%
        unlist()
      
      balanced_accuracy <- sum(recall) / length(recall)
    }
    
    # Calculate accuracy
    
    accuracy <- sum(diag(cm)) / sum(cm)
    
    if(use_balanced_accuracy){
      mainOuts <- data.frame(accuracy = accuracy, 
                             balanced_accuracy = balanced_accuracy)
    } else{
      mainOuts <- data.frame(accuracy = accuracy)
    }
    
    mainOuts <- mainOuts%>%
      dplyr::mutate(category = "Main")
    
  } else{
    
    if (use_balanced_accuracy) {
      fitControl <- caret::trainControl(method = "none",
                                        summaryFunction = calculate_balanced_accuracy,
                                        classProbs = TRUE)
    } else {
      fitControl <- caret::trainControl(method = "none",
                                        classProbs = TRUE)
    }
    
    mod <- caret::train(group ~ .,
                        data = tmp_train,
                        method = test_method,
                        trControl = fitControl,
                        preProcess = c("center", "scale", "nzv"))
    
    # Print variable importance summary
    
    importance <<- c(importance, caret::varImp(mod, scale = FALSE))
    
    # Get main predictions
    
    u <- dplyr::union(predict(mod, newdata = tmp_test), tmp_test$group)
    mytable <- table(factor(stats::predict(mod, newdata = tmp_test), u), factor(tmp_test$group, u))
    cm <- t(as.matrix(caret::confusionMatrix(mytable)$table))
    
    if(use_balanced_accuracy){
      
      recall <- 1:nrow(cm) %>%
        purrr::map(~ calculate_recall(cm, x = .x)) %>%
        unlist()
      
      balanced_accuracy <- sum(recall) / length(recall)
    }
    
    # Calculate accuracy
    
    accuracy <- sum(diag(cm)) / sum(cm)
    
    if(use_balanced_accuracy){
      mainOuts <- data.frame(accuracy = accuracy, 
                             balanced_accuracy = balanced_accuracy)
    } else{
      mainOuts <- data.frame(accuracy = accuracy)
    }
    
    mainOuts <- mainOuts
  }
  
  mainOuts <- mainOuts %>%
    dplyr::mutate(resample = x)
  
  return(mainOuts)
}

#--------------
# Model fitting
#--------------

fit_multi_feature_models <- function(data, test_method, use_balanced_accuracy, use_k_fold, num_folds, num_resamples = 30, set = NULL){
  
  # Set up input matrices
  
  if(!is.null(set)){
    
    message(paste0("\nCalculating models for ", set))
    
    tmp_mods <- data %>%
      dplyr::filter(.data$method == set)
    
  } else{
    tmp_mods <- data
  }
  
  # Get number of cases in each set
  
  train_rows <- tmp_mods %>%
    dplyr::filter(.data$set_split == "Train") %>%
    dplyr::select(c(.data$id)) %>%
    dplyr::distinct() %>%
    nrow()
  
  test_rows <- tmp_mods %>%
    dplyr::filter(.data$set_split == "Test") %>%
    dplyr::select(c(.data$id)) %>%
    dplyr::distinct() %>%
    nrow()
  
  # Get proportion per class in train and test to use for resample procedure
  
  train_props <- tmp_mods %>%
    dplyr::filter(.data$set_split == "Train") %>%
    dplyr::select(c(.data$id, .data$group)) %>%
    dplyr::distinct() %>%
    dplyr::group_by(.data$group) %>%
    dplyr::summarise(counter = dplyr::n()) %>%
    dplyr::ungroup()
  
  test_props <- tmp_mods %>%
    dplyr::filter(.data$set_split == "Test") %>%
    dplyr::select(c(.data$id, .data$group)) %>%
    dplyr::distinct() %>%
    dplyr::group_by(.data$group) %>%
    dplyr::summarise(counter = dplyr::n()) %>%
    dplyr::ungroup()
  
  # Run model fitting via resampling
  
  finalOuts <- 1:num_resamples %>%
    purrr::map_df(~ fit_resamples(data = tmp_mods, 
                                  train_rows = train_rows, 
                                  test_rows = test_rows, 
                                  train_groups = train_props, 
                                  test_groups = test_props, 
                                  x = .x, 
                                  test_method = test_method, 
                                  use_balanced_accuracy = use_balanced_accuracy,
                                  use_k_fold = use_k_fold, 
                                  num_folds = num_folds))
  
  # Return final dataframe
  
  if(!is.null(set)){
    finalOuts <- finalOuts %>%
      dplyr::mutate(method = set,
                    num_features_used = length(unique(tmp_mods$names)))
  }
  
  return(finalOuts)
}

#---------------
# 'where' helper
#---------------

# NOTE: This is from {tidyselect} but due to import limitations for CRAN (and it not being namespaced) it's rebuilt here
# See https://github.com/r-lib/tidyselect/blob/main/R/helpers-where.R for implementation

mywhere <- function(fn) {
  predicate <- rlang::as_function(fn)
  
  function(x, ...) {
    out <- predicate(x, ...)
    
    if (!rlang::is_bool(out)) {
      rlang::abort("`where()` must be used with functions that return `TRUE` or `FALSE`.")
    }
    
    out
  }
}

#------------------------------
# Pre-processing by feature set
#------------------------------

clean_by_set <- function(data, themethod = NULL){
  
  if(is.null(themethod)){
    tmp_cleaner <- data
    themethod <- "matrix of all features"
  } else{
    tmp_cleaner <- data %>%
      dplyr::filter(.data$method == themethod)
  }
  
  # Widening for model matrix
  
  tmp_cleaner <- tmp_cleaner %>%
    dplyr::mutate(names = paste0(.data$method, "_", .data$names)) %>%
    dplyr::select(-c(.data$method)) %>%
    tidyr::pivot_wider(id_cols = c("id", "group", "set_split"), names_from = "names", values_from = "values")
  
  ncols <- ncol(tmp_cleaner)
  
  # Delete features that are all NaNs and features with constant values
  
  tmp_cleaner <- tmp_cleaner %>%
    dplyr::select_if(~sum(!is.na(.)) > 0) %>%
    dplyr::select(mywhere(~dplyr::n_distinct(.) > 1))
  
  if(ncol(tmp_cleaner) < ncols){
    message(paste0("Dropped ", ncols - ncol(tmp_cleaner), "/", ncol(tmp_cleaner), " features from ", themethod, " due to containing NAs or only a constant."))
  }
  
  # Check NAs
  
  nrows <- nrow(tmp_cleaner)
  
  tmp_cleaner <- tmp_cleaner %>%
    tidyr::drop_na()
  
  if(nrow(tmp_cleaner) < nrows){
    message(paste0("Dropped ", nrows - nrow(tmp_cleaner), " unique IDs due to NA values."))
  }
  
  # Clean up column (feature) names so models fit properly (mainly an issue with SVM formula) and re-join set labels
  # and prep factor levels as names for {caret} if the 3 base two-class options aren't being used
  
  tmp_cleaner <- tmp_cleaner %>%
    janitor::clean_names() %>%
    tidyr::pivot_longer(cols = 4:ncol(tmp_cleaner), names_to = "names", values_to = "values") %>%
    dplyr::mutate(method = gsub("_.*", "\\1", .data$names)) %>%
    dplyr::mutate(group = as.factor(.data$group)) %>%
    dplyr::mutate(group = as.integer(group),
                  group = paste0("Group_", group),
                  group = make.names(.data$group),
                  group = as.factor(.data$group))
  
  return(tmp_cleaner)
}

#---------------- Main function ----------------

#' Modified version of the \code{theft} function to fit a classifier to feature matrix using all features or all features by set
#' @importFrom rlang .data as_function is_bool abort
#' @import dplyr
#' @import ggplot2
#' @importFrom tidyr drop_na pivot_wider pivot_longer
#' @importFrom tibble rownames_to_column
#' @importFrom stats sd reorder ecdf pnorm
#' @importFrom purrr map map_df
#' @importFrom janitor clean_names
#' @importFrom caret preProcess train confusionMatrix
#' @param data the dataframe containing the raw feature data as calculated by \code{theft::calculate_features}
#' @param id_var a string specifying the ID variable to group data on (if one exists). Defaults to \code{"id"}
#' @param group_var a string specifying the grouping variable that the data aggregates to. Defaults to \code{"group"}
#' @param by_set Boolean specifying whether to compute classifiers for each feature set. Defaults to \code{FALSE}
#' @param test_method the algorithm to use for quantifying class separation. Defaults to \code{"gaussprRadial"}
#' @param use_balanced_accuracy a Boolean specifying whether to use balanced accuracy as the summary metric for caret model training. Defaults to \code{FALSE}
#' @param use_k_fold a Boolean specifying whether to use k-fold procedures for generating a distribution of classification accuracy estimates. Defaults to \code{TRUE}
#' @param num_folds an integer specifying the number of folds (train-test splits) to perform if \code{use_k_fold} is set to \code{TRUE}. Defaults to \code{10}
#' @param num_resamples an integer specifying the number of resamples to compute. Defaults to \code{30}
#' @return an object of class list containing dataframe summaries of the classification models and a \code{ggplot} object if \code{by_set} is \code{TRUE}
#' @author Trent Henderson
#' @export
#' @examples
#' \donttest{
#' featMat <- calculate_features(data = simData,
#'   id_var = "id",
#'   time_var = "timepoint",
#'   values_var = "values",
#'   group_var = "process",
#'   feature_set = "catch22",
#'   seed = 123)
#'
#' fit_multi_feature_classifier_tt(featMat,
#'   id_var = "id",
#'   group_var = "group",
#'   by_set = FALSE,
#'   test_method = "gaussprRadial",
#'   use_balanced_accuracy = FALSE,
#'   use_k_fold = TRUE,
#'   num_folds = 10,
#'   num_resamples = 30)
#' }
#'

fit_multi_feature_classifier_tt <- function(data, id_var = "id", group_var = "group",
                                            by_set = FALSE, test_method = "gaussprRadial",
                                            use_balanced_accuracy = FALSE, use_k_fold = TRUE, 
                                            num_folds = 10, num_resamples = 30){
  
  #------------- Renaming columns -------------
  
  if (is.null(id_var)){
    stop("Data is not uniquely identifiable. Please add a unique identifier variable.")
  }
  
  if(!is.null(id_var)){
    data_id <- data %>%
      dplyr::rename(id = dplyr::all_of(id_var),
                    group = dplyr::all_of(group_var)) %>%
      dplyr::select(c(.data$id, .data$group, .data$set_split, .data$method, .data$names, .data$values))
  }
  
  #------------- Preprocess data --------------
  
  # NOTE: This performs NA checking and filtering by feature set to maximise features and IDs for each if `by_set = TRUE`
  
  if(by_set){
    
    message("Assessing feature values and IDs by individual set.")
    
    data_id <- unique(data_id$method) %>%
      purrr::map_df(~ clean_by_set(data = data_id, themethod = .x))
  } else{
    message("Assessing feature values and IDs using matrix of all features.")
    data_id <- clean_by_set(data = data_id, themethod = NULL)
  }
  
  #------------- Fit models -------------------
  
  #---------------------
  # Set up useful method
  # information
  #---------------------
  
  classifier_name <- test_method
  statistic_name <- ifelse(use_balanced_accuracy, "Mean classification accuracy and balanced classification accuracy", "Mean classification accuracy")
  
  if(by_set){
    
    sets <- unique(data_id$method)
    
    # Compute accuracies for each feature set
    
    output <- sets %>%
      purrr::map_df(~ fit_multi_feature_models(data = data_id,
                                               test_method = test_method,
                                               use_balanced_accuracy = use_balanced_accuracy,
                                               use_k_fold = use_k_fold,
                                               num_folds = num_folds,
                                               num_resamples = num_resamples,
                                               set = .x))
    
  } else{
    
    output <- fit_multi_feature_models(data = data_id,
                                       test_method = test_method,
                                       use_balanced_accuracy = use_balanced_accuracy,
                                       use_k_fold = use_k_fold,
                                       num_folds = num_folds,
                                       num_resamples = num_resamples,
                                       set = NULL)
  }
  
  #--------------- Return results ---------------
  
  # NOTE: Removed barplot and statistical testing functionality here in {theft} as we don't need it for this project
  # Just shortens the code up since we are doing different comparative testing later on
  
  output <- output %>%
    dplyr::mutate(classifier_name = classifier_name,
                  statistic_name = statistic_name)
  
  return(output)
}
```

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Retain just PCs and class labels

tt_labels <- tmp %>%
  dplyr::select(c(id, set_split)) %>%
  distinct() %>%
  mutate(id = as.character(id))

X <- fits %>%
  broom::augment(fit_mat) %>%
  dplyr::select(c(".rownames", .fittedPC1:.fittedPC19)) %>%
  rename(id = .rownames) %>%
  left_join(groups, by = c("id" = "id")) %>%
  left_join(tt_labels, by = c("id" = "id")) %>%
  clean_names() %>%
  pivot_longer(cols = fitted_pc1:fitted_pc19, names_to = "names", values_to = "values") %>%
  mutate(method = "PCA")

# Set up storage for variable importances

importance <- list()

# Fit random forest models

results <- fit_multi_feature_classifier_tt(X, 
                                           id_var = "id", 
                                           group_var = "group_id",
                                           by_set = FALSE, 
                                           test_method = "rf", 
                                           use_balanced_accuracy = TRUE,
                                           use_k_fold = TRUE, 
                                           num_folds = 10, 
                                           num_resamples = 30)
```

XX

```{r, echo = FALSE, warning = FALSE, message = FALSE}

```

XX

See Figure \@ref(fig:impranks). See Figure \@ref(fig:impdists).

```{r impranks, fig.cap = "Frequency of ranks over all resamples are plotted for each principal component used as a predictor in the models. $PC 2$ demonstrates the highest number of first rankings, indicating that across the 30 resamples (different train-test splits), $PC 2$ is more often than not the most informative predictor of class ($Weekend$ versus $Weekday$).", echo = FALSE, warning = FALSE, message = FALSE}
# Wrangle variable importance

get_imps <- function(thelist, x){
  mydf <- data.frame(importance = thelist[x]$importance$Overall) %>%
    dplyr::mutate(pc = paste0("PC ", dplyr::row_number()))
  
  return(mydf)
}

importances <- 1:length(importance) %>%
  purrr::map_df(~ get_imps(thelist = importance, x = .x))

imp_ranks <- importances %>%
  group_by(pc) %>%
  mutate(resample = row_number()) %>%
  group_by(resample) %>%
  mutate(ranker = dense_rank(-importance)) %>%
  group_by(pc, ranker) %>%
  summarise(counter = n()) %>%
  ungroup() %>%
  mutate(pc = factor(pc, levels = c("PC 1", "PC 2", "PC 3", "PC 4", "PC 5",
                                    "PC 6", "PC 7", "PC 8", "PC 9", "PC 10",
                                    "PC 11", "PC 12", "PC 13", "PC 14", "PC 15",
                                    "PC 16", "PC 17", "PC 18", "PC 19"))) %>%
  ggplot(aes(x = ranker, y = counter)) +
  geom_bar(stat = 'identity', fill = "#1B9E77", alpha = 0.9) +
  labs(title = "Frequency of variable importance ranks over all resamples",
       x = "Rank",
       y = "Frequency") +
  ggplot2::theme_bw() +
  ggplot2::theme(panel.grid.minor = ggplot2::element_blank(),
                 legend.position = "bottom",
                 plot.title = element_text(face = "bold"),
                 strip.background = element_blank()) +
  facet_wrap(~pc)

print(imp_ranks)
```

```{r impdists, fig.cap = "Mean variable importance $\\pm$ 1SD is plotted for each principal component used as predictors in the models. $PC 2$ demonstrates the highest mean variable importance values, but with a large variance.", echo = FALSE, warning = FALSE, message = FALSE}
# Generate distributions of importance

imp_dists <- importances %>%
  group_by(pc) %>%
  summarise(mu = mean(importance, na.rm = TRUE),
            lower = mu - sd(importance, na.rm = TRUE),
            upper = mu + sd(importance, na.rm = TRUE)) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(pc, mu), y = mu)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), colour = "#1B9E77", size = 1) +
  geom_point(colour = "#1B9E77", size = 2) +
  labs(title = "Distribution of variable importance values over all resamples",
       x = "Principal component",
       y = "Variable importance") +
  coord_flip() +
  ggplot2::theme_bw() +
  ggplot2::theme(panel.grid.minor = ggplot2::element_blank(),
                 legend.position = "bottom",
                 plot.title = element_text(face = "bold"),
                 strip.background = element_blank())

print(imp_dists)
```

