---
title: OLET5610 Report
authors:
  - name: Trent Henderson
    email: then6675@uni.sydney.edu.au
bibliography: references.bib
biblio-style: unsrt
output: 
  bookdown::pdf_book:
    base_format: rticles::arxiv_article
---

# Method

Time-series classification algorithms are typically benchmarked using the Time Series Classification Repository, which contains 128 univariate (i.e., values sampled uniformly in time) time-series classification datasets/problems and 30 multivariate (i.e., for any time $t$, $\mathbf{y}_{t} = (y_{1t},\ldots, y_{nt})$ describes $n$ realisations at time $t$) datasets [@UEA_UCR_Repo]. Here, we focus on the univariate setting. Many algorithms developed across the sciences have been evaluated across the standardised univariate datasets provided in the repository, with black-box algorithms typically outperforming competitors [@bagnallGreatTimeSeries2017]. Given the growing need for algorithmic transparency in the sciences and industry, the present research aims to explore the feasibility of a new approach using one of the 128 datasets as a test case: the "Wafer" dataset [@wafer_data]. Specifically, this work aims to determine if using informative summary statistics (known as "features"), can be used to construct a high performing classification procedure that rivals or outperforms existing approaches [@fulcherHighlyComparativeTimeseries2013; @fulcherFeaturebasedTimeseriesAnalysis2017; @fulcherFeatureBasedTimeSeriesAnalysis2018]. Examples of time-series features include properties of the time-series distribution of values, autocorrelation structure, entropy, model-fit statistics, nonlinear time-series analysis, stationarity, and many others [@fulcherHighlyComparativeFeaturebased2014]. At its core, feature-based time-series analysis reduces an time series $\times$ time matrix to a time series $\times$ feature matrix, which can then be used for statistical learning (such as classification), using the values of time-series features as inputs.

## Dataset

The "Wafer" dataset in the Time Series Classification Repository contains a collection of inline process control measurements that were recorded from a range of sensors during the manufacturing and processing of silicon wafers for semiconductor fabrication [@wafer_data]. Each unique time series in the Wafer dataset represents the measurements of one sensor during the processing of one wafer by one tool. Labels for two classes are provided in the data: *Normal* and *Abnormal*^[https://www.timeseriesclassification.com/description.php?Dataset=Wafer]. The goal of this problem is to predict class membership from the time-series values. The dataset contains a pre-designated train-test split, with 1000 samples in the train set (each of length $T = 152$), and 6164 samples in the test set (each of length $T = 152$).

## Algorithmic Approach

The approach in this work contains three stages: (i) extraction of time-series features for each unique time series; (ii) dimensionality reduction using principal components analysis (PCA) to obtain a reduced set of informative vectors; and (iii) classification using the top principal components (quantified in terms of variance explained) as inputs into a random forest classifier. Features from two open-source feature sets (**catch22** [@lubbaCatch22CAnonicalTimeseries2019; @Rcatch22] and **Kats** [@Kats]) will be extracted using the R package "theft" [@theft]. **catch22** extracts 24 features (as mean and standard deviation are added to the standard 22 features) and **Kats** extracts 40 features. This will produce a resulting time series $\times$ feature matrix of size 7164 $\times$ 64. Given the existence of within-set redundancy (i.e., high absolute correlations between features in a set) observed particularly for **Kats** relative to **catch22** and between-set redundancy identified in previous work [@hendersonEmpiricalEvaluationTimeSeries2021], dimensionality reduction through PCA will be applied to substantially reduce the input matrix size for the classification algorithm into a time series $\times$ principal component matrix. A threshold of 80$\%$ cumulative variance explained will be used to determine the number of principal components to retain. Following the procedure of previous work [@ruizGreatMultivariateTime2021], the methodology presented here then trains and evaluates the accuracy of the classifier over 30 resamples of train-test splits, where each is seeded for reproducibility, and the first is always the pre-designated train-test split in the data as it comes from the Time Series Classification Repository. This will enable scientific inference of algorithmic performance with uncertainty, and facilitate a direct comparison with the performance of existing algorithms and benchmarks.

Given the existence of the Time Series Classification Repository [@UEA_UCR_Repo] which holds the Wafer dataset (whose sole purpose is to facilitate benchmarking of time-series classification algorithms), it is expected that meaningful class separation will be possible. The present research takes a novel approach of chaining time-series feature extraction, dimensionality reduction techniques, and a classification algorithm --- an approach that has seen almost no research attention to-date. As such, a primary goal of this work is to understand the performance of this approach relative to current benchmarks. 

Given the high within-set redundancy (i.e., high absolute correlations between features in a set) observed in previous work, it is hypothesised that dimensionality reduction techniques will substantially reduce the input matrix size for the classification algorithm from the original time series $\times$ feature matrix to a much smaller time series $\times$ principal component matrix [@hendersonEmpiricalEvaluationTimeSeries2021]. Further, it is also hypothesised that using the time series $\times$ principal component matrix as input to a classification algorithm (i.e., random forest classifier) will not result in a substantial reduction in classification performance compared to using the time series $\times$ feature matrix nor to existing benchmarks due to the highly informative nature of time-series features in understanding temporal dynamics [@fulcherHighlyComparativeTimeseries2013].

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Load packages

library(dplyr)
library(tidyr)
library(ggplot2)
library(tibble)
library(foreign)
library(factoextra)
library(ggpubr)
library(broom)
library(janitor)
library(caret)
library(viridis)

# Load the dataset (pre-run in olet5610/report/OLET5610 Report/calculate-features.R to avoid Python/Rmarkdown issues)

load("tmp.Rda")
load("outs.Rda")
```

# Results

Prior to substantative analysis, exploratory data analysis was performed. A sample of three time series from each class (*Normal* and *Abnormal*) is displayed in Figure \@ref(fig:tsplots). To the eye, some small, but noticeable differences in temporal dynamics and shape are visible between the classes, which suggests that classification based on temporal properties (i.e, "features") is a feasible approach.

```{r tsplots, fig.cap = "Raw time-series plots of three randomly selected time series from each class ($Normal$ and $Abnormal$) in the Wafer dataset. Small differences in temporal dynamics are visible.", echo = FALSE, warning = FALSE, message = FALSE}
# Plot some raw time series examples (three from each class)

set.seed(123)

ids_1 <- tmp %>%
  filter(target == "1") %>%
  dplyr::select(id) %>%
  distinct() %>%
  pull(id) %>%
  sample(size = 3)

ids_2 <- tmp %>%
  filter(target == "-1") %>%
  dplyr::select(id) %>%
  distinct() %>%
  pull(id) %>%
  sample(size = 3)

tsplot <- tmp %>%
  filter(id %in% append(ids_1, ids_2)) %>%
  dplyr::mutate(target = ifelse(.data$target == "1", "Normal", "Abnormal")) %>%
  dplyr::mutate(target = factor(.data$target, levels = c("Normal", "Abnormal")),
                id = paste0("ID: ", id)) %>%
  dplyr::mutate(id = factor(id, levels = c("ID: 1237", "ID: 1934", "ID: 4839",
                                           "ID: 2738", "ID: 2791", "ID: 2471"))) %>%
  ggplot(aes(x = timepoint, y = values, colour = target)) +
  geom_line(size = 0.7) +
  labs(title = "Raw time series samples from both classes",
       x = "Time",
       y = "Value",
       colour = NULL) +
  scale_colour_brewer(palette = "Dark2") +
  theme_bw() +
  theme(legend.position = "bottom",
        panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"),
        strip.background = element_blank(),
        strip.text = element_text(face = "bold")) +
  facet_wrap(~id)

print(tsplot)
```

## Dimensionality reduction

Time-series features were computed from two sets (**catch22** and **Kats**) on all time series, which produced a time series $\times$ feature matrix of size 7164 $\times$ 64. This matrix is large, which increases computation time for classification algorithms and increases complexity of model interpretation and evaluation. To reduce this complexity, a principal components analysis was performed. The dataset meets the assumptions of PCA, as multicollinearity between time-series features was a reason for conducting the analysis, and there are sufficient samples to adequately perform PCA as the $7164:64$ $samples:variables$ ratio exceeds the $20:1$ recommendation [@osborneSampleSizeSubject2019]. 

Figure \@ref(fig:eigenplots) plots a visual summary of the PCA. Panel **(A)** plots the percentage of variance explained for the top eight principal components (PC) which together explain 80$\%$ of the variance. $PC 1$ explains $37.5\%$ of the variance, but there is a steep drop off following this component, with $PC 2$ explaining $15.6\%$ of the variance. Panel **(B)** plots cumulative variance explained for the eight retained PCs. The first four PCs explain two-thirds of the variance in the dataset. Panel **(C)** plots the eigenvalues of the eight PCs. All eight PCs exceed the $\lambda = 1$ threshold for the Kaiser criterion of PC retention [@Kaiser1960].

```{r eigenplots, fig.cap = "Summary of eight retained principal components. (A) Percentage of variance explained is plotted in descending order for each of the retained principal components. (B) Cumulative variance explained is plotted for each of the retained principal components. An 80% cumulative variance threshold was selected to determine the principal components to retain which returned the eight plotted here (from the original 64). (C) Eigenvalues of the eight retained principal components are plotted in descending order. All retained components also exceed the $\\lambda = 1$ cutoff for the Kaiser criterion.", echo = FALSE, warning = FALSE, message = FALSE, fig.width = 7, fig.height = 8}
#-------
# Do PCA
#-------

# Widen model matrix and fit PCA

mywhere <- function(fn) {
  predicate <- rlang::as_function(fn)
  
  function(x, ...) {
    out <- predicate(x, ...)
    if (!rlang::is_bool(out)) {
      rlang::abort("`where()` must be used with functions that return `TRUE` or `FALSE`.")
    }
    out
  }
}

fit_mat <- outs %>%
  dplyr::select(id, names, values) %>%
  tidyr::pivot_wider(id_cols = "id", names_from = "names", values_from = "values") %>%
  tibble::column_to_rownames(var = "id") %>%
  dplyr::select_if(~sum(!is.na(.)) > 0) %>% # Remove features that are just NA before dropping rows
  tidyr::drop_na() %>%
  dplyr::select(mywhere(~dplyr::n_distinct(.) > 1)) # Keep columns that aren't a constant

# Get feature matrix size to highlight dimensionality

#print(paste0(nrow(fit_mat), " x ", ncol(fit_mat)))

# Fit PCA

fits <- fit_mat %>%
  stats::prcomp(scale = TRUE)

#------------------------
# Assess # of PCs to keep
#------------------------

# Filter to just PCs that contribute just over 80% of the variance

eigs <- as.data.frame(get_eig(fits)) %>%
  tibble::rownames_to_column(var = "dimension") %>%
  mutate(rank = dense_rank(cumulative.variance.percent),
         flag = ifelse(cumulative.variance.percent > 80, TRUE, FALSE)) %>%
  group_by(flag) %>%
  mutate(ranker = dense_rank(cumulative.variance.percent)) %>%
  ungroup() %>%
  mutate(ranker = ifelse(flag == FALSE, 1, ranker)) %>%
  filter(ranker == 1) %>%
  dplyr::select(-c(rank, flag, ranker)) %>%
  mutate(dimension = gsub("Dim.", "\\1", dimension),
         dimension = paste0("PC ", dimension))

# Draw % of variance explained plot

p <- eigs %>%
  ggplot(aes(x = reorder(dimension, -variance.percent), y = variance.percent)) +
  geom_bar(stat = "identity", fill = "#1B9E77", alpha = 0.9) +
  geom_line(group = 1, size = 1) +
  geom_point(size = 2) +
  labs(title = "(A) Percentage of variance explained by principal component",
       x = "Principal component",
       y = "Variance explained (%)") +
  geom_line() +
  geom_point() +
  scale_y_continuous(limits = c(0, 40),
                     breaks = seq(from = 0, to = 40, by = 5),
                     labels = function(x)paste(x, "%")) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"),
        axis.text = element_text(size = 8))

# Draw cumulative variance plot

p1 <- eigs %>%
  ggplot(aes(x = reorder(dimension, cumulative.variance.percent), y = cumulative.variance.percent)) +
  geom_bar(stat = "identity", fill = "#D95F02", alpha = 0.9) +
  geom_line(group = 1, size = 1) +
  geom_point(size = 2) +
  geom_hline(yintercept = 80, lty = "dashed", colour = "black", size = 0.8) +
  labs(title = "(B) Cumulative variance explained by principal components",
       x = "Principal component",
       y = "Cumulative variance explained (%)") +
  scale_y_continuous(limits = c(0, 100),
                     breaks = seq(from = 0, to = 100, by = 20),
                     labels = function(x)paste(x, "%")) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"),
        axis.text = element_text(size = 8))

# Draw eigenvalue plot

p2 <- eigs %>%
  ggplot(aes(x = reorder(dimension, -eigenvalue), y = eigenvalue)) +
  geom_bar(stat = "identity", fill = "#7570B3", alpha = 0.9) +
  geom_hline(yintercept = 1, lty = "dashed", colour = "black", size = 0.8) +
  labs(title = "(C) Eigenvalues of retained principal components",
       x = "Principal component",
       y = "Eigenvalue\n") +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        plot.title = element_text(face = "bold"),
        axis.text = element_text(size = 8))

# Combine plots

eigenplots <- ggpubr::ggarrange(p, p1, p2, nrow = 3, ncol = 1, common.legend = TRUE, legend = "bottom")
print(eigenplots)
```

Loadings for each of the 64 time-series features (variables) on the eight PCs is presented in Figure \@ref(fig:loadplot). Patterns are evident across the PCs, such as the strong loading of time-series features associated with properties of the autocorrelation and partial autocorrelation function onto $PC 1$, and histogram-based statistics onto $PC 3$. This plot gives us interpretable and informative insight into the relative behaviour of the time-series features extracted from the Wafer dataset.

```{r loadplot, fig.cap = "Loadings of each variable onto the eight retained principal components. Relationships between the time-series features are visible, such as loading of features associated with the autocorrelation and partial autocorrelation function onto $PC 1$.", fig.width = 7, echo = FALSE, warning = FALSE, message = FALSE}
contribs <- get_pca_var(fits)

methods <- outs %>%
  dplyr::select(c(method, names)) %>%
  distinct() %>%
  mutate(full_name = paste0(stringr::str_to_upper(method), "_", names))

loadings_plot <- as.data.frame(contribs$contrib) %>%
  dplyr::select(c(Dim.1:Dim.8)) %>%
  tibble::rownames_to_column(var = "names") %>%
  left_join(methods, by = c("names" = "names")) %>%
  dplyr::select(c(full_name, Dim.1:Dim.8)) %>%
  pivot_longer(cols = Dim.1:Dim.8, names_to = "names", values_to = "value") %>%
  mutate(names = gsub("Dim.", "PC ", names)) %>%
  mutate(names = factor(names, levels = c("PC 1", "PC 2", "PC 3", "PC 4", "PC 5", "PC 6", "PC 7", "PC 8"))) %>%
  ggplot2::ggplot(ggplot2::aes(x = .data$names, y = .data$full_name, fill = .data$value))  +
  ggplot2::geom_tile() +
  ggplot2::labs(title = "Variable loadings onto principal components",
                x = "Principal component",
                y = "Variable",
                fill = "Component loading") +
  viridis::scale_fill_viridis(option = "plasma") +
  ggplot2::theme_bw() + 
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1, size = 8),
                 axis.text.y = ggplot2::element_text(size = 4),
                 axis.ticks.y = ggplot2::element_blank(),
                 legend.position = "bottom",
                 panel.grid.minor = element_blank(),
                 plot.title = element_text(face = "bold"))

print(loadings_plot)
```

Prior to modelling, the PCA was distilled into an even lower dimensional space of just two dimensions to understand if class differences could be ascertained with just the two PCs which explain the most variance in the data (a collective $53.1\%$). This is displayed in Figure \@ref(fig:biplot)). There is considerable overlap between the $Normal$ and $Abnormal$ classes in the two-dimensional space, suggesting the additional six principal components are likely needed for accurate classification.

```{r biplot, fig.cap = "Principal components analysis biplot. The first principal component (positioned along the $x$-axis) explains $37.5\\%$ of the variance in the Wafer dataset. The second principal component (positioned along the $y$-axis) explains $15.6\\%$ of the variance in the Wafer dataset. Class-level covariance is displayed as shaded ellipses.", echo = FALSE, warning = FALSE, message = FALSE}
# Get eigenvalues and summary stats for plot

eigen_summary <- fits %>%
  broom::tidy(matrix = "eigenvalues") %>%
  dplyr::filter(.data$PC %in% c(1, 2)) %>% # Filter to just the 2 going in the plot
  dplyr::select(c(.data$PC, .data$percent)) %>%
  dplyr::mutate(percent = round(.data$percent * 100), digits = 1)

eigen_pc1 <- eigen_summary %>%
  dplyr::filter(.data$PC == 1)

eigen_pc2 <- eigen_summary %>%
  dplyr::filter(.data$PC == 2)

eigen_pc1 <- paste0(eigen_pc1$percent,"%")
eigen_pc2 <- paste0(eigen_pc2$percent,"%")

# Get group data

groups <- outs %>%
  dplyr::rename(group_id = group) %>%
  dplyr::group_by(.data$id, .data$group_id) %>%
  dplyr::summarise(counter = dplyr::n()) %>%
  dplyr::ungroup() %>%
  dplyr::select(-c(.data$counter)) %>%
  dplyr::mutate(id = as.factor(.data$id)) %>%
  dplyr::mutate(group_id = ifelse(.data$group_id == "1", "Normal", "Abnormal")) %>%
  dplyr::mutate(group_id = factor(.data$group_id, levels = c("Normal", "Abnormal")))

# Draw plot

pca_plot <- fits %>%
  broom::augment(fit_mat) %>%
  dplyr::rename(id = 1) %>%
  dplyr::mutate(id = as.factor(.data$id)) %>%
  dplyr::rename(.fitted1 = .data$.fittedPC1,
                .fitted2 = .data$.fittedPC2) %>%
  dplyr::inner_join(groups, by = c("id" = "id")) %>%
  dplyr::mutate(group_id = as.factor(.data$group_id)) %>%
  ggplot2::ggplot(ggplot2::aes(x = .data$.fitted1, y = .data$.fitted2)) +
  ggplot2::stat_ellipse(ggplot2::aes(x = .data$.fitted1, y = .data$.fitted2, fill = .data$group_id), geom = "polygon", alpha = 0.2) +
  ggplot2::guides(fill = "none") +
  ggplot2::scale_fill_brewer(palette = "Dark2") +
  ggplot2::geom_point(size = 1.25, ggplot2::aes(colour = .data$group_id)) + 
  ggplot2::labs(title = "Principal components analysis biplot",
                x = paste0("PC 1"," (", eigen_pc1, ")"),
                y = paste0("PC 2"," (", eigen_pc2, ")"),
                colour = NULL) +
  ggplot2::scale_colour_brewer(palette = "Dark2") +
  ggplot2::theme_bw() +
  ggplot2::theme(panel.grid.minor = ggplot2::element_blank(),
                 legend.position = "bottom",
                 plot.title = element_text(face = "bold"),
                 axis.text = element_text(size = 8),
                 axis.title = element_text(size = 9))

print(pca_plot)
```

## Time-series classification

```{r, echo = FALSE, warning = FALSE, message = FALSE}
#--------------- Helper functions ----------------

#-----------------------
# Classification metrics
#-----------------------

# Recall (for use in computing balanced classification accuracy)

calculate_recall <- function(matrix, x){
  tp <- as.numeric(matrix[x, x])
  fn <- sum(matrix[x, -x])
  
  # Add a catch for when 0s occupy the entire row in the confusion matrix
  # NOTE: Is this the correct way to handle? Seems consistent with {caret}'s default matrix
  
  if(tp + fn == 0){
    recall <- 0
  } else{
    recall <- tp / (tp + fn)
  }
  return(recall)
}

# Four MECE parts of the confusion matrix (TP, FP, TN, FN)

calculate_cm_stats <- function(matrix, x){
  tp <- as.numeric(matrix[x, x])
  fp <- sum(matrix[-x, x])
  tn <- sum(matrix[-x, -x])
  fn <- sum(matrix[x, -x])
  mymat <- matrix(c(tp, fp, tn, fn), nrow = 1, ncol = 4)
  return(mymat)
}

#-------------------------------------
# Calculate balanced accuracy in caret
#-------------------------------------

calculate_balanced_accuracy <- function(data, lev = NULL, model = NULL) {
  
  # Calculate balanced accuracy from confusion matrix as the average of class recalls as per https://arxiv.org/pdf/2008.05756.pdf
  
  cm <- t(as.matrix(caret::confusionMatrix(data$pred, data$obs)$table))
  
  recall <- 1:nrow(cm) %>%
    purrr::map(~ calculate_recall(cm, x = .x)) %>%
    unlist()
  
  balanced_accuracy <- sum(recall) / length(recall)
  
  # Calculate accuracy
  
  accuracy <- sum(diag(cm)) / sum(cm)
  
  # Return results
  
  out <- c(accuracy, balanced_accuracy)
  names(out) <- c("Accuracy", "Balanced_Accuracy")
  return(out)
}

#-----------------------
# Resample model fitting
#-----------------------

#' Function to fit models over 30 resamples as per Bagnall et al. and save results where first sample is always original train-test split
#' @param data the dataframe of feature results to operate on
#' @param train_rows number of cases in the train set
#' @param test_rows number of cases in the test set
#' @param train_groups dataframe containing proportions of each class in original train split
#' @param test_groups dataframe containing proportions of each class in original test split
#' @param x the resample number to control seed for reproducibility
#' @param test_method the algorithm to use for quantifying class separation. Defaults to \code{"gaussprRadial"}
#' @param use_balanced_accuracy a Boolean specifying whether to use balanced accuracy as the summary metric for caret model training. Defaults to \code{FALSE}
#' @param use_k_fold a Boolean specifying whether to use k-fold procedures for generating a distribution of classification accuracy estimates. Defaults to \code{TRUE}
#' @param num_folds an integer specifying the number of folds (train-test splits) to perform if \code{use_k_fold} is set to \code{TRUE}. Defaults to \code{10}
#' @returns an object of class dataframe
#' @author Trent Henderson
#' 

fit_resamples <- function(data, train_rows, test_rows, train_groups, test_groups, x, 
                          test_method, use_balanced_accuracy, use_k_fold, num_folds){
  
  message(paste0("Fitting model ", x))
  set.seed(x)
  
  #----------------- Data preparation and resampling ------------------
  
  if(x == 1){
    
    # Set up train and test data
    
    tmp_train <- data %>%
      dplyr::filter(.data$set_split == "Train") %>%
      dplyr::select(-c(.data$set_split)) %>%
      dplyr::select(-c(.data$method)) %>%
      tidyr::pivot_wider(id_cols = c("id", "group"), names_from = "names", values_from = "values") %>%
      dplyr::select(-c(.data$id))
    
    tmp_test <- data %>%
      dplyr::filter(.data$set_split == "Test") %>%
      dplyr::select(-c(.data$set_split)) %>%
      dplyr::select(-c(.data$method)) %>%
      tidyr::pivot_wider(id_cols = c("id", "group"), names_from = "names", values_from = "values") %>%
      dplyr::select(-c(.data$id))
    
  } else{
    
    # Randomly allocate the correct number of each class to train and test as manual stratified sample
    
    resampled <- list()
    
    for(i in unique(data$group)){
      feasible_ids <- data %>%
        dplyr::filter(group == i) %>%
        dplyr::select(c(id)) %>%
        dplyr::distinct() %>%
        dplyr::pull(id)
      
      n <- train_groups %>%
        dplyr::filter(group == i) %>%
        pull(counter)
      
      traindata <- data.frame(id = sample(feasible_ids, size = n)) %>%
        dplyr::mutate(set_split_new = "Train")
      
      testdata <- data.frame(id = feasible_ids[!feasible_ids %in% traindata$id]) %>%
        dplyr::mutate(set_split_new = "Test")
      
      stopifnot((nrow(traindata) + nrow(testdata)) == length(feasible_ids))
      
      joined <- dplyr::bind_rows(traindata, testdata)
      resampled[[i]] <- joined
    }
    
    resampled <- do.call(rbind, resampled)
    rownames(resampled) <- c()
    
    # Properly set up train and test data
    
    newdata <- data %>%
      dplyr::left_join(resampled, by = c("id" = "id"))
    
    tmp_train <- newdata %>%
      dplyr::filter(.data$set_split_new == "Train") %>%
      dplyr::select(c(.data$id, .data$group, .data$names, .data$values)) %>%
      tidyr::pivot_wider(id_cols = c("id", "group"), names_from = "names", values_from = "values") %>%
      dplyr::select(-c(.data$id))
    
    tmp_test <- newdata %>%
      dplyr::filter(.data$set_split_new == "Test") %>%
      dplyr::select(c(.data$id, .data$group, .data$names, .data$values)) %>%
      tidyr::pivot_wider(id_cols = c("id", "group"), names_from = "names", values_from = "values") %>%
      dplyr::select(-c(.data$id))
  }
  
  #----------------- Model fitting and returns ------------------
  
  # Fit models
  
  if(use_k_fold){
    
    # Train model
    
    if (use_balanced_accuracy) {
      fitControl <- caret::trainControl(method = "cv",
                                        number = num_folds,
                                        summaryFunction = calculate_balanced_accuracy,
                                        classProbs = TRUE)
    } else {
      fitControl <- caret::trainControl(method = "cv",
                                        number = num_folds,
                                        classProbs = TRUE)
    }
    
    mod <- caret::train(group ~ .,
                        data = tmp_train,
                        method = test_method,
                        trControl = fitControl,
                        preProcess = c("center", "scale", "nzv"))
    
    # Print variable importance summary
    
    importance <<- c(importance, caret::varImp(mod, scale = FALSE))
    
    # Get main predictions
    
    u <- dplyr::union(predict(mod, newdata = tmp_test), tmp_test$group)
    mytable <- table(factor(stats::predict(mod, newdata = tmp_test), u), factor(tmp_test$group, u))
    cm <- t(as.matrix(caret::confusionMatrix(mytable)$table))
    
    if(use_balanced_accuracy){
      
      recall <- 1:nrow(cm) %>%
        purrr::map(~ calculate_recall(cm, x = .x)) %>%
        unlist()
      
      balanced_accuracy <- sum(recall) / length(recall)
    }
    
    # Calculate accuracy
    
    accuracy <- sum(diag(cm)) / sum(cm)
    
    if(use_balanced_accuracy){
      mainOuts <- data.frame(accuracy = accuracy, 
                             balanced_accuracy = balanced_accuracy)
    } else{
      mainOuts <- data.frame(accuracy = accuracy)
    }
    
    mainOuts <- mainOuts%>%
      dplyr::mutate(category = "Main")
    
  } else{
    
    if (use_balanced_accuracy) {
      fitControl <- caret::trainControl(method = "none",
                                        summaryFunction = calculate_balanced_accuracy,
                                        classProbs = TRUE)
    } else {
      fitControl <- caret::trainControl(method = "none",
                                        classProbs = TRUE)
    }
    
    mod <- caret::train(group ~ .,
                        data = tmp_train,
                        method = test_method,
                        trControl = fitControl,
                        preProcess = c("center", "scale", "nzv"))
    
    # Print variable importance summary
    
    importance <<- c(importance, caret::varImp(mod, scale = FALSE))
    
    # Get main predictions
    
    u <- dplyr::union(predict(mod, newdata = tmp_test), tmp_test$group)
    mytable <- table(factor(stats::predict(mod, newdata = tmp_test), u), factor(tmp_test$group, u))
    cm <- t(as.matrix(caret::confusionMatrix(mytable)$table))
    
    if(use_balanced_accuracy){
      
      recall <- 1:nrow(cm) %>%
        purrr::map(~ calculate_recall(cm, x = .x)) %>%
        unlist()
      
      balanced_accuracy <- sum(recall) / length(recall)
    }
    
    # Calculate accuracy
    
    accuracy <- sum(diag(cm)) / sum(cm)
    
    if(use_balanced_accuracy){
      mainOuts <- data.frame(accuracy = accuracy, 
                             balanced_accuracy = balanced_accuracy)
    } else{
      mainOuts <- data.frame(accuracy = accuracy)
    }
    
    mainOuts <- mainOuts
  }
  
  mainOuts <- mainOuts %>%
    dplyr::mutate(resample = x)
  
  return(mainOuts)
}

#--------------
# Model fitting
#--------------

fit_multi_feature_models <- function(data, test_method, use_balanced_accuracy, use_k_fold, num_folds, num_resamples = 30, set = NULL){
  
  # Set up input matrices
  
  if(!is.null(set)){
    
    message(paste0("\nCalculating models for ", set))
    
    tmp_mods <- data %>%
      dplyr::filter(.data$method == set)
    
  } else{
    tmp_mods <- data
  }
  
  # Get number of cases in each set
  
  train_rows <- tmp_mods %>%
    dplyr::filter(.data$set_split == "Train") %>%
    dplyr::select(c(.data$id)) %>%
    dplyr::distinct() %>%
    nrow()
  
  test_rows <- tmp_mods %>%
    dplyr::filter(.data$set_split == "Test") %>%
    dplyr::select(c(.data$id)) %>%
    dplyr::distinct() %>%
    nrow()
  
  # Get proportion per class in train and test to use for resample procedure
  
  train_props <- tmp_mods %>%
    dplyr::filter(.data$set_split == "Train") %>%
    dplyr::select(c(.data$id, .data$group)) %>%
    dplyr::distinct() %>%
    dplyr::group_by(.data$group) %>%
    dplyr::summarise(counter = dplyr::n()) %>%
    dplyr::ungroup()
  
  test_props <- tmp_mods %>%
    dplyr::filter(.data$set_split == "Test") %>%
    dplyr::select(c(.data$id, .data$group)) %>%
    dplyr::distinct() %>%
    dplyr::group_by(.data$group) %>%
    dplyr::summarise(counter = dplyr::n()) %>%
    dplyr::ungroup()
  
  # Run model fitting via resampling
  
  finalOuts <- 1:num_resamples %>%
    purrr::map_df(~ fit_resamples(data = tmp_mods, 
                                  train_rows = train_rows, 
                                  test_rows = test_rows, 
                                  train_groups = train_props, 
                                  test_groups = test_props, 
                                  x = .x, 
                                  test_method = test_method, 
                                  use_balanced_accuracy = use_balanced_accuracy,
                                  use_k_fold = use_k_fold, 
                                  num_folds = num_folds))
  
  # Return final dataframe
  
  if(!is.null(set)){
    finalOuts <- finalOuts %>%
      dplyr::mutate(method = set,
                    num_features_used = length(unique(tmp_mods$names)))
  }
  
  return(finalOuts)
}

#---------------
# 'where' helper
#---------------

# NOTE: This is from {tidyselect} but due to import limitations for CRAN (and it not being namespaced) it's rebuilt here
# See https://github.com/r-lib/tidyselect/blob/main/R/helpers-where.R for implementation

mywhere <- function(fn) {
  predicate <- rlang::as_function(fn)
  
  function(x, ...) {
    out <- predicate(x, ...)
    
    if (!rlang::is_bool(out)) {
      rlang::abort("`where()` must be used with functions that return `TRUE` or `FALSE`.")
    }
    
    out
  }
}

#------------------------------
# Pre-processing by feature set
#------------------------------

clean_by_set <- function(data, themethod = NULL){
  
  if(is.null(themethod)){
    tmp_cleaner <- data
    themethod <- "matrix of all features"
  } else{
    tmp_cleaner <- data %>%
      dplyr::filter(.data$method == themethod)
  }
  
  # Widening for model matrix
  
  tmp_cleaner <- tmp_cleaner %>%
    dplyr::mutate(names = paste0(.data$method, "_", .data$names)) %>%
    dplyr::select(-c(.data$method)) %>%
    tidyr::pivot_wider(id_cols = c("id", "group", "set_split"), names_from = "names", values_from = "values")
  
  ncols <- ncol(tmp_cleaner)
  
  # Delete features that are all NaNs and features with constant values
  
  tmp_cleaner <- tmp_cleaner %>%
    dplyr::select_if(~sum(!is.na(.)) > 0) %>%
    dplyr::select(mywhere(~dplyr::n_distinct(.) > 1))
  
  if(ncol(tmp_cleaner) < ncols){
    message(paste0("Dropped ", ncols - ncol(tmp_cleaner), "/", ncol(tmp_cleaner), " features from ", themethod, " due to containing NAs or only a constant."))
  }
  
  # Check NAs
  
  nrows <- nrow(tmp_cleaner)
  
  tmp_cleaner <- tmp_cleaner %>%
    tidyr::drop_na()
  
  if(nrow(tmp_cleaner) < nrows){
    message(paste0("Dropped ", nrows - nrow(tmp_cleaner), " unique IDs due to NA values."))
  }
  
  # Clean up column (feature) names so models fit properly (mainly an issue with SVM formula) and re-join set labels
  # and prep factor levels as names for {caret} if the 3 base two-class options aren't being used
  
  tmp_cleaner <- tmp_cleaner %>%
    janitor::clean_names() %>%
    tidyr::pivot_longer(cols = 4:ncol(tmp_cleaner), names_to = "names", values_to = "values") %>%
    dplyr::mutate(method = gsub("_.*", "\\1", .data$names)) %>%
    dplyr::mutate(group = as.factor(.data$group)) %>%
    dplyr::mutate(group = as.integer(group),
                  group = paste0("Group_", group),
                  group = make.names(.data$group),
                  group = as.factor(.data$group))
  
  return(tmp_cleaner)
}

#---------------- Main function ----------------

#' Modified version of the \code{theft} function to fit a classifier to feature matrix using all features or all features by set
#' @importFrom rlang .data as_function is_bool abort
#' @import dplyr
#' @import ggplot2
#' @importFrom tidyr drop_na pivot_wider pivot_longer
#' @importFrom tibble rownames_to_column
#' @importFrom stats sd reorder ecdf pnorm
#' @importFrom purrr map map_df
#' @importFrom janitor clean_names
#' @importFrom caret preProcess train confusionMatrix
#' @param data the dataframe containing the raw feature data as calculated by \code{theft::calculate_features}
#' @param id_var a string specifying the ID variable to group data on (if one exists). Defaults to \code{"id"}
#' @param group_var a string specifying the grouping variable that the data aggregates to. Defaults to \code{"group"}
#' @param by_set Boolean specifying whether to compute classifiers for each feature set. Defaults to \code{FALSE}
#' @param test_method the algorithm to use for quantifying class separation. Defaults to \code{"gaussprRadial"}
#' @param use_balanced_accuracy a Boolean specifying whether to use balanced accuracy as the summary metric for caret model training. Defaults to \code{FALSE}
#' @param use_k_fold a Boolean specifying whether to use k-fold procedures for generating a distribution of classification accuracy estimates. Defaults to \code{TRUE}
#' @param num_folds an integer specifying the number of folds (train-test splits) to perform if \code{use_k_fold} is set to \code{TRUE}. Defaults to \code{10}
#' @param num_resamples an integer specifying the number of resamples to compute. Defaults to \code{30}
#' @return an object of class list containing dataframe summaries of the classification models and a \code{ggplot} object if \code{by_set} is \code{TRUE}
#' @author Trent Henderson
#' @export
#' @examples
#' \donttest{
#' featMat <- calculate_features(data = simData,
#'   id_var = "id",
#'   time_var = "timepoint",
#'   values_var = "values",
#'   group_var = "process",
#'   feature_set = "catch22",
#'   seed = 123)
#'
#' fit_multi_feature_classifier_tt(featMat,
#'   id_var = "id",
#'   group_var = "group",
#'   by_set = FALSE,
#'   test_method = "gaussprRadial",
#'   use_balanced_accuracy = FALSE,
#'   use_k_fold = TRUE,
#'   num_folds = 10,
#'   num_resamples = 30)
#' }
#'

fit_multi_feature_classifier_tt <- function(data, id_var = "id", group_var = "group",
                                            by_set = FALSE, test_method = "gaussprRadial",
                                            use_balanced_accuracy = FALSE, use_k_fold = TRUE, 
                                            num_folds = 10, num_resamples = 30){
  
  #------------- Renaming columns -------------
  
  if (is.null(id_var)){
    stop("Data is not uniquely identifiable. Please add a unique identifier variable.")
  }
  
  if(!is.null(id_var)){
    data_id <- data %>%
      dplyr::rename(id = dplyr::all_of(id_var),
                    group = dplyr::all_of(group_var)) %>%
      dplyr::select(c(.data$id, .data$group, .data$set_split, .data$method, .data$names, .data$values))
  }
  
  #------------- Preprocess data --------------
  
  # NOTE: This performs NA checking and filtering by feature set to maximise features and IDs for each if `by_set = TRUE`
  
  if(by_set){
    
    message("Assessing feature values and IDs by individual set.")
    
    data_id <- unique(data_id$method) %>%
      purrr::map_df(~ clean_by_set(data = data_id, themethod = .x))
  } else{
    message("Assessing feature values and IDs using matrix of all features.")
    data_id <- clean_by_set(data = data_id, themethod = NULL)
  }
  
  #------------- Fit models -------------------
  
  #---------------------
  # Set up useful method
  # information
  #---------------------
  
  classifier_name <- test_method
  statistic_name <- ifelse(use_balanced_accuracy, "Mean classification accuracy and balanced classification accuracy", "Mean classification accuracy")
  
  if(by_set){
    
    sets <- unique(data_id$method)
    
    # Compute accuracies for each feature set
    
    output <- sets %>%
      purrr::map_df(~ fit_multi_feature_models(data = data_id,
                                               test_method = test_method,
                                               use_balanced_accuracy = use_balanced_accuracy,
                                               use_k_fold = use_k_fold,
                                               num_folds = num_folds,
                                               num_resamples = num_resamples,
                                               set = .x))
    
  } else{
    
    output <- fit_multi_feature_models(data = data_id,
                                       test_method = test_method,
                                       use_balanced_accuracy = use_balanced_accuracy,
                                       use_k_fold = use_k_fold,
                                       num_folds = num_folds,
                                       num_resamples = num_resamples,
                                       set = NULL)
  }
  
  #--------------- Return results ---------------
  
  # NOTE: Removed barplot and statistical testing functionality here in {theft} as we don't need it for this project
  # Just shortens the code up since we are doing different comparative testing later on
  
  output <- output %>%
    dplyr::mutate(classifier_name = classifier_name,
                  statistic_name = statistic_name)
  
  return(output)
}
```

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Retain just PCs and class labels

tt_labels <- tmp %>%
  dplyr::select(c(id, set_split)) %>%
  distinct() %>%
  mutate(id = as.character(id))

X <- fits %>%
  broom::augment(fit_mat) %>%
  dplyr::select(c(".rownames", .fittedPC1:.fittedPC8)) %>%
  rename(id = .rownames) %>%
  left_join(groups, by = c("id" = "id")) %>%
  left_join(tt_labels, by = c("id" = "id")) %>%
  janitor::clean_names() %>%
  pivot_longer(cols = fitted_pc1:fitted_pc8, names_to = "names", values_to = "values") %>%
  mutate(method = "PCA")

# Set up storage for variable importances

importance <- list()

# Fit random forest models

results <- fit_multi_feature_classifier_tt(X, 
                                           id_var = "id", 
                                           group_var = "group_id",
                                           by_set = FALSE, 
                                           test_method = "rf", 
                                           use_balanced_accuracy = TRUE,
                                           use_k_fold = TRUE, 
                                           num_folds = 10, 
                                           num_resamples = 30)
```

The time series $\times$ principal component matrix (7164 $\times$ 8) was passed as input into a random forest classifier using the **caret** package in R [@caret] over 30 resamples, where the first sample was the pre-designated train-test split from the Time Series Classification Repository. Mean classification accuracy across the resamples was $99.08\% (SD=0.19\%)$ and is compared to previous benchmarks [@bagnallGreatTimeSeries2017] in Table \@ref(tab:comptable). The benchmark algorithms include collective of transformation-based ensembles (COTE), shapelet transform (ST), bag of SFA symbols (BOSS), elastic ensemble (EE), dynamic time warping (DTW), time series forest (TSF), time series bag of features (TSBF), learned pattern (LPS), and move-split-merge (MSM). The current approach, while marginally outperformed by the other algorithms, demonstrates classification performance that can be considered to be on-par with more complex, more black-box algorithms, despite only using eight principal components as an input to a random forest classifier. Importantly, no manual hyperparameter tuning was performed, beyond **caret's** basic parameter grid search that it performs over the $k$-fold cross-validation procedure. It is likely that even stronger performance could be obtained through more detailed hyperparameter tuning and optimisation. Further, relative to the benchmark algorithms presented in Table \@ref(tab:comptable), the current approach is *fast*, executing the entire classification stage (including all 30 resamples, each with 10-fold cross-validation) in under four minutes locally on a laptop with no parallel processing.

```{r comptable, echo = FALSE, warning = FALSE, message = FALSE}
comp_table <- data.frame(Algorithm = c("Current approach", "COTE", "ST", "BOSS", 
                                       "EE", "DTW", "TSF", "TSBF", "LPS", "MSM"),
                         mean_acc = c(round(mean(results$accuracy), digits = 3), 
                                             0.999, 1.000, 0.999, 
                                             0.997, 0.996, 0.997, 0.996, 0.995, 0.996)) %>%
  mutate(mean_acc = mean_acc * 100) %>%
  arrange(-mean_acc) %>%
  mutate(mean_acc = paste0(mean_acc, "%")) %>%
  rename(`Mean Accuracy` = mean_acc)

knitr::kable(comp_table, caption = "Comparison of mean classification accuracy results on the Wafer dataset between the current approach and existing benchmarks. Performance differences are minimal between all approaches, with each algorithm achieving $\\>99\\%$ accuracy.", format = "latex")
```

To better understand the current approach, we examine the machinery of the random forest models. Understanding relative variable importance can shed a deeper light into random forest models to aid interpretability. Variable importance ranks for each variable (principal components) across the 30 resamples are plotted in Figure \@ref(fig:impranks). $PC 4$ (comprised of mostly symbolic features, such as those associated with entropy of small-set probabilities, flat spots, and proportion of magnitudes that exceed a threshold over the standard deviation) is the most important variable across all 30 resamples for predicting class ($Normal$ versus $Abnormal$). The performance of $PC 4$ is further demonstrated in Figure \@ref(fig:impdists) which plots the mean $\pm1SD$ of variable importance values across the 30 resamples. On average, $PC 4$ exhibits variable importance of a factor of 5.5 higher than any other variable.

```{r impranks, fig.cap = "Frequency of ranks over all resamples are plotted for each principal component used as a predictor in the models. $PC 4$ is the most important variable across all 30 resamples for predicting class ($Normal$ versus $Abnormal$).", echo = FALSE, warning = FALSE, message = FALSE}
# Wrangle variable importance

get_imps <- function(thelist, x){
  mydf <- data.frame(importance = thelist[x]$importance$Overall) %>%
    dplyr::mutate(pc = paste0("PC ", dplyr::row_number()))
  
  return(mydf)
}

importances <- 1:length(importance) %>%
  purrr::map_df(~ get_imps(thelist = importance, x = .x))

imp_ranks <- importances %>%
  group_by(pc) %>%
  mutate(resample = row_number()) %>%
  group_by(resample) %>%
  mutate(ranker = dense_rank(-importance)) %>%
  group_by(pc, ranker) %>%
  summarise(counter = n()) %>%
  ungroup() %>%
  mutate(pc = factor(pc, levels = c("PC 1", "PC 2", "PC 3", "PC 4", "PC 5",
                                    "PC 6", "PC 7", "PC 8"))) %>%
  ggplot(aes(x = ranker, y = counter)) +
  geom_bar(stat = 'identity', fill = "#1B9E77", alpha = 0.9) +
  labs(title = "Frequency of variable importance ranks over all resamples",
       x = "Rank",
       y = "Frequency") +
  ggplot2::theme_bw() +
  ggplot2::theme(panel.grid.minor = ggplot2::element_blank(),
                 legend.position = "bottom",
                 plot.title = element_text(face = "bold"),
                 strip.background = element_blank()) +
  facet_wrap(~pc)

print(imp_ranks)
```

```{r impdists, fig.cap = "Mean variable importance $\\pm$ 1SD is plotted for each principal component used as predictors in the models. $PC 4$ demonstrates the highest mean variable importance values by a factor of 5.5 over the next highest component ($PC 6$).", echo = FALSE, warning = FALSE, message = FALSE}
# Generate distributions of importance

imp_dists <- importances %>%
  group_by(pc) %>%
  summarise(mu = mean(importance, na.rm = TRUE),
            lower = mu - sd(importance, na.rm = TRUE),
            upper = mu + sd(importance, na.rm = TRUE)) %>%
  ungroup() %>%
  ggplot(aes(x = reorder(pc, mu), y = mu)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), colour = "#1B9E77", size = 0.8) +
  geom_point(colour = "#1B9E77", size = 1.5) +
  labs(title = "Distribution of variable importance over all resamples",
       x = "Principal component",
       y = "Variable importance value") +
  coord_flip() +
  ggplot2::theme_bw() +
  ggplot2::theme(panel.grid.minor = ggplot2::element_blank(),
                 legend.position = "bottom",
                 plot.title = element_text(face = "bold"),
                 strip.background = element_blank())

print(imp_dists)
```

# References
